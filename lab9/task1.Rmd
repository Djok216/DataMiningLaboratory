```{r}

data = read.csv('../data/train.csv')
data = data[-1]

```

```{r}
# k folding
k <- 5  # one partition for test and rest for train

folds <- sample(rep(1:k, length.out = nrow(data)))

split_data <- function(ith){
  if (ith > k || ith <= 0)
    stop("Inexistent partition")
  
  pelem <- which(folds == ith)
  return (list("test" = data[pelem, ], "train" = data[-pelem, ]))
}
```

```{r}
# decision tree classifier
library(rpart)
library(caret)

partition <- split_data(4)
test = partition$test
train = partition$train
tr <- rpart(formula=Cover_Type ~ ., data=train, method="class")
rpart.plot(tr, box.palette = 0)
printcp(tr)
cm <- table(test$Cover_Type, rpart.predict(tr, test, type="class"))
rownames(cm) <- paste("Actual", rownames(cm), sep = ":")
colnames(cm) <- paste("Pred", colnames(cm), sep = ":")
cm
sum(diag(cm)) / nrow(test)
```

```{r}
library(caret)
library(rpart.plot)

intrain <- createDataPartition(y = data$Cover_Type, p= 0.7, list = FALSE)
training <- data[intrain,]
testing <- data[-intrain,]
dim(training); dim(testing);

decisinStump(X=training$Elevation, Y=training$Id)
# trctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
# dtree_fit <- train(Cover_Type ~., data = training, method = "gbm",
#                    parms = list(split = "information"),
#                    trControl=trctrl,
#                    tuneLength = 10)
# test_pred <- predict(dtree_fit, newdata = testing, type="prob")
# print("\n")
# str(test_pred)
# str(as.vector(testing$Cover_Type))
# 
# 
# confusionMatrix(as.vector(testing$Cover_Type), as.vector(testing$Cover_Type))
names(getModelInfo())
```

```{r}
#install.packages("drat", repos = "https://cran.rstudio.com")
#drat::addRepo("dmlc")
#cran <- getOption("repos")
#cran["dmlc"] <- "https://s3-us-west-2.amazonaws.com/apache-mxnet/R/CRAN/"
#options(repos = cran)
#install.packages("mxnet",dependencies = T)
library(mxnet)
data = read.csv('../data/train_withoutheader.csv')
data = data[-1]

partition = split_data(4)
test.x = t(partition$test[, 1:(length(partition$test)-1)])
test.y = partition$test[, length(partition$test)] - 1
train.x = t(partition$train[, 1:(length(partition$train)-1)])
train.y = partition$train[, length(partition$train)] - 1

m1.data = mx.symbol.Variable("data")

m1.fc1 = mx.symbol.FullyConnected(m1.data, name="fc1", num_hidden=256)
m1.act1 = mx.symbol.Activation(m1.fc1, name="act", act_type="softrelu")

m1.fc2 = mx.symbol.FullyConnected(m1.act1, name="fc2", num_hidden=128)
m1.act2 = mx.symbol.Activation(m1.fc2, name="act", act_type="softrelu")

m1.fc3 = mx.symbol.FullyConnected(m1.act2, name="fc3", num_hidden=64)
m1.act3 = mx.symbol.Activation(m1.fc3, name="act", act_type="softrelu")

m1.fc4 = mx.symbol.FullyConnected(m1.act3, name="fc4", num_hidden=32)
m1.act4 = mx.symbol.Activation(m1.fc4, name="act4", act_type="softrelu")

m1.fc5 = mx.symbol.FullyConnected(m1.act4, name="fc5", num_hidden=16)
m1.softmax = mx.symbol.SoftmaxOutput(m1.fc5, name="softmax")

ick <- proc.time()
mx.set.seed(0)
m1 = mx.model.FeedForward.create(m1.softmax,
                                     X = train.x,
                                     y = train.y,
                                     num.round = 10,
                                     array.batch.size = 256,
                                     array.layout="colmajor",
                                     optimizer='adam',
                                     eval.metric = mx.metric.accuracy,
                                     initializer = mx.init.uniform(0.07)
)
m1.preds = predict(m1, test.x, array.layout = "colmajor")

```

```{r}
library(caret)
library(xgboost)
data = read.csv('../data/train_withoutheader.csv')
data = data[-1]

fileName = 'xgboost_results.txt'
fileObj = file(fileName)
write('', file=fileObj, append=F)
close(fileObj)
for (i in 1:1) {
  partition = split_data(i)
  test.x = partition$test[, 1:(length(partition$test)-1)]
  test.y = partition$test[, length(partition$test)] - 1
  to_train.x = partition$train[, 1:(length(partition$train)-1)]
  to_train.y = partition$train[, length(partition$train)] - 1
  
  xgb.grid = expand.grid(
          nrounds=100,
          max_depth=c(5,10,15),
          eta=c(0.3,0.025,0.0025),
          gamma=c(0.501,1.0),
          colsample_bytree=1,
          min_child_weight=1
          )
  
  xgb.opt.mod = xgboost(data=xgb.DMatrix(model.matrix(~.,data=to_train.x),label=to_train.y),
                           num_class=7,
                           nrounds=2,
                           params=list(objective="multi:softmax",eval_metric="merror",allowParallel=T),
                           tuneGrid=xgb.grid,
                           method='xgbTree'
                        )
  
  saveRDS(xgb.opt.mod,"xgb.opt.mod")
  xgb.opt.pred = predict(xgb.opt.mod,newdata=xgb.DMatrix(model.matrix(~.,data=test.x),label=test.y))
  confusionMatrix(factor(xgb.opt.pred, levels=0:6),factor(test.y, levels=0:6))
  fileName = 'xgboost_results.txt'
  writeLines(toString(factor(test.y, levels=0:6)), fileName)
  #writeLines(toString(factor(xgb.opt.pred, levels=0:6)), fileName)
}
```

